<!DOCTYPE html> 
<html lang="en">
<head>

	<meta charset="UTF-8">
	<title>Beyond Known Objects: Prompting 6D Pose Estimator to Learn Novel Objects Continually</title>

	<link rel="stylesheet" type="text/css" href="layout.css">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<style>

		h1 {
			font-size: 200%;
			text-align: center;
		}

		h2 {
			font-size: 150%;
			text-align: center;
		}

		h3 {
			font-size: 100%;
			text-align: center;
		}

		.center-justified {
			text-align: justify;
			margin: 0 auto;
			width: 1000px;
		}

		.center {
			text-align: center;
		}

	/* NAVIGATION */
	nav {0
		width: 100%;
		margin: 0 auto;
		background: #fff;
	}

	/* By Dominik Biedebach @domobch */
	nav ul {
		list-style: none;
		text-align: center;
	}
	nav ul li {
		display: inline-block;
	}
	nav ul li a {
		display: block;
		padding: 0px;
		text-decoration: none;
		color: #aaa;
		font-weight: 800;
		margin: 0 10px;
	}
	nav ul li a,
	nav ul li a:after,
	nav ul li a:before {
		transition: all .5s;
	}
	nav ul li a:hover {
		color: #555;
	}

	/* stroke */
	nav.stroke ul li a,
	nav.fill ul li a {
		position: relative;
	}
	nav.stroke ul li a:after,
	nav.fill ul li a:after {
		position: absolute;
		bottom: 0;
		left: 0;
		right: 0;
		margin: auto;
		width: 0%;
		content: '.';
		color: transparent;
		background: #333;
		height: 1px;
	}
	nav.stroke ul li a:hover:after {
		width: 100%;
	}

    .data-table {
        border-collapse: collapse;
    }
    .border-top {
        border-top: 1px solid #000;
    }
    .border-bottom {
        border-bottom: 1px solid #000;
    }
    .border-left {
        border-left: 1px solid #000;
    }
    .border-right {
        border-right: 1px solid #000;
    }
</style>

<script async src='/cdn-cgi/bm/cv/669835187/api.js'></script></head>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>

<body>
	<div class="center-justified" style="width:1000px;"> 
		<h1>Beyond Known Objects: Prompting 6D Pose Estimator to Learn Novel Objects Continually</h1>
		<!--
		<h3>Long Tian <sup>1 2</sup>, Amelia Sorrenti<sup>3</sup>, Yik Lung Pang<sup>2</sup>, Giovanni Bellitto<sup>3</sup>, Simone Palazzo<sup>3</sup>, Concetto Spampinato<sup>3</sup>, and Changjae Oh<sup>3</sup></h3>
		<h3> <sup>1</sup> Southwest Jiaotong University, China </h3>
		<h3> <sup>2</sup> Queen Mary University of London, United Kingdom </h3>
		<h3> <sup>3</sup> University of Catania, Italy </h3>
		-->
	</div>
	<div class="center-justified">
		<p>
				<b>ABSTRACT:</b> Robust 6D object pose estimation is essential for enabling intelligent
			systems to interact with the physical world. While existing methods have achieved notable progress,
			they remain constrained by two major limitations: poor scalability when new objects are introduced and
			severe performance degradation caused by catastrophic forgetting in sequential learning settings.
			To address these challenges, we propose a novel prompt-based continual learning framework for 6D pose
			estimation. Our approach leverages frozen foundation features for robust appearance and geometric
			representation, while introducing a multi-head pose predictor to isolate task-specific regressors and a
			prompt pool that serves as a rehearsal-free knowledge repository. The proposed framework enables
			task-agnostic inference by retrieving relevant prompts and activating the appropriate predictor without
			requiring explicit task labels. We conduct comprehensive experiments on the Linemod and YCB-Video datasets
			under task-incremental settings. Results show that our method achieves competitive performance compared
			with state-of-the-art baselines, while exhibiting strong robustness to task order, adaptability to
			arbitrary object groupings, and high scalability through lightweight predictor expansion. To the best of
			our knowledge, this work represents the first systematic study of continual learning for 6D pose
			estimation, providing a foundation toward scalable and adaptive open-world perception.
        </p>
		<br>
		<br>


<!--	<div style="text-align: center;">-->
<!--		<img width=1000 src="framework/framework.png">-->
<!--	</div>-->

	<h2 class="Motivation" style="text-align: left;">Motivation</h2>
	<div style="text-align: center;">
		<img width=600 src="motivation.png">
	</div>
		<p>
			<b>Motivation:</b> We explore several model in the domain of 6Dof pose estimation on the Linemod dataset benchmark.This
			figure shows that the state-of-art models can achieve an average accuracy rate of 99.7% on
			the evaluation of ADD(S)-0.1d, and this performance is already very close to 100%. Therefore, there
			is only a 0.3% space for improvement. It should be noted that these models with excellent performance
			are usually the results obtained by training all objects in a single task on the entire dataset.
			What if we adopt another perspective to view this issue and train object by object in a
			streaming manner? Taking the Densefusion model as a comparison, the green line represents the result
			of training on the whole objects, and the red line represents the result of streaming training object
			by object. As we can see, when we train the tasks in a streaming manner instead of a
			single task, the model only fits the current object perfectly, and there is catastrophic forgetting for the objects
			trained previously, except for the object of glue, which is symmetric object. Mitigating this
			catastrophic forgetting and filling this unexplored area are our main motivations. And it is the way to
			achieve artificial general intelligence.
		</p>
		<br>
		<br>

	<h2 class="Framework" style="text-align: left;">Framework</h2>
	<div style="text-align: center;">
		<img width=800 src="framework.png">
	</div>
		<p>
			Our framework operates in two stages. In the first stage, an image and its corresponding point cloud are processed by vision and geometry foundation models, respectively. The extracted visual and geometric features are fused to form point-wise representations, which are then passed to a multi-head pose predictor to estimate the 6D pose. In the second stage, only the image is used as input. A subset of prompts is retrieved from a prompt pool via a cosine similarity-based selection mechanism and prepended to the patch embeddings. These enriched embeddings are fed into a classifier to determine the task identity, which in turn specifies the appropriate pose predictor to be employed during training and inference.
		</p>
		<br>
		<br>

	<h2 class="Pseudo Code" style="text-align: left;">Pseudo Code</h2>
		<div style="text-align: center;">
			<img width=800 src="pseudo-code.png">
		</div>
		<!--
		<p>
			Initially, we exclusively use data from the new object to enable rapid adjustment of the network's
			trainable parameters to suit the characteristics of the new object. After processing all new
			object data, we evaluate network performance on memory buffer data to detect any significant drop
			in performance on previously seen objects. If the loss value exceeds a predefined threshold,
			indicating a decline in performance, we incorporate memory buffer data for training. Conversely,
			if the loss value remains below the threshold, signifying retention of previously encountered
			objects, we refrain from adding memory buffer data to training to prevent overfitting.
		</p>
		-->
		Our proposed method mainly consists of two stages, at the training process, we feed the object's image crop
		and it's corresponding point cloud data of current task into the first stage to output a pose estimation, which composed of frozen feature
		extractors and a Multi-head pose predictor to mitigate the forgetting. Meanwhile, we feed the image crop into the second
		stage, which contains a prompt pool (shared memory buffer) and a classifier to identify the task identity, which will be used to determine
		the appropriate pose predictor in the first stage during inference.
		<br>
		<br>

	<h2 class="Tracking results" style="text-align: left;">Qualitative results on 6Dof pose estimation continual learning</h2>
	<div style="text-align: center;">
		<video width="900" controls autoplay muted loop style="vertical-align: middle;"><source src="linemod_ape.mp4" type="video/mp4" >
		</video>
	</div>

	<div style="text-align: center;">
		<video width="900" controls autoplay muted loop style="vertical-align: middle;"><source src="linemod_drill.mp4" type="video/mp4" >
		</video>
	</div>

	<div style="text-align: center;">
		<video width="900" controls autoplay muted loop style="vertical-align: middle;"><source src="ycb1.mp4" type="video/mp4" >
		</video>
	</div>

	<div style="text-align: center;">
		<video width="900" controls autoplay muted loop style="vertical-align: middle;"><source src="ycb2.mp4" type="video/mp4" >
		</video>
	</div>


<!--
	<h2 style="text-align: left;">Contacts</h2>
	<p>
		If you have any further enquiries, question, or comments, please contact 
		<a href="mailto:long.tian@qmul.ac.uk"><u>long.tian@swjtu.edu.cn</u></a>.
	</p>
-->
		</div>
	<br>
	<br>
<script type="text/javascript">(function(){window['__CF$cv$params']={r:'6de861896b0275cb',m:'ppqRX6tRQH4fUS12qEp4ASn7HTYLZcxczg_G238ujv8-1645031338-0-ASgYPROdw9+o4jFoUNr8Id49tDTtlx5gflbKrAkhfXy2ENQyKulR5gLqJ0Bkdz59YBZm/firsQF0/kj1XO5yyxa5ap7YlCONhhMEOMs9TeGlQzoXAMk4nHBNsniUUaUcRTdaSZhpPQGOa6qn4k9yqil4uoTx4WWfMhmspNBlh1UhCVyy0G3RKeZWSAkf/VI9eQ==',s:[0x1f8bab9e4e,0x98dc696c79],}})();</script></body> 
</html>
